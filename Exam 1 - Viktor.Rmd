---
title: "Exam 1 - Viktor"
author: "Viktor Damm"
date: "2024-05-02"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(sandwich)
library(foreign)
library(texreg)
```

```{r}
data1 <- read.csv("~/Desktop/Oecon/4. semester/Økonometri/Econ Exam/data1.csv")
salary<-data1$salary
educ<-data1$educ
salbegin<-data1$salbegin
male<-data1$male
minority<-data1$minority
```

log(salary) = β0 + β1educ + β2log(salbegin) + β3male + β4minority + u

#1. Estimer modellen vha. OLS. Kommenter på outputtet og fortolk resultaterne.

First of all, we are going the estimate the model using a simple OLS regression in R. This is done with the following code:
```{r}
reg=lm(log(salary)~educ+log(salbegin)+male+minority)
summary(reg)
```
First, we are going to comment of the estimates: 

We can see that if education increases by one unit, then salary increases by 2,3 pct, holding every other parameter fixed. Furthermore, we can see that the estimate is significant down to a 0.1 pct. significance level. Next, we can see that a 1 pct. increase in beginning salaly leads to a 0,82 pct. increase in salary. This estimate is significant down to a 0.1 pct. significance level, as well. We have also included two dummy variables in our regression, male and minority. The estimate on male is 0.04, meaning that a male gets 4% higher salary than females (which is the reference group) holding everything else fixed. The estimate on minority is -0.04, which means that a minority gets 4% lower salary than non-minorities (which is the reference group) holding everything else fixed. Both of these estiamtes are significant down to a 5 pct. significance level. 

We can observe that all variables have the expected sign on the estimates. Further, the R-squared and adjusted R-squared are both around 80 pct, meaning that the model fits 80% of the data. The R-square indicates how well the model fits the data, meaning that it is a meausre of how good the model is. The adjusted R-sqaure also takes the number of independent variables into account. The F-statistic in the model is 481,3 with a p-value of 2.2e-16, meaning that the varibles are jointly significant. 

#Opgave 2. - Udfør grafisk modelkontrol

To answer the quastion we make 4 different graphic model control, because we find theese the most interesting for doing graphical control of the model:
We make graphical control of the model to conclude wether the Gauss Markov assumptions are fullfiled.

Firstly we plot the residuals agains the fitted values, to look at if it seems that there is a non-linear relationsship in the model:

```{r}
plot(reg, 1)
```
Since the dots are equally spread around a horizontal line, which indicates that we dont have any non-linear relationship, which we should account. 
This is a sign of a well specified model, which means that there is no variables which should be included in quadratic form and thereby in a non-linear form.


Next we look at the distribution of the error term. We do this by plotting the standard residuals against the theoretical quantiles, and also in a histrogram of residuals. Lastly we test it manually using the jarque bera test:

```{r}
plot(reg, 2)
```
It seems that the residuals fit nicely around the dottet line, which means that they are normally distributed. This indicates that MLR 6 is fullfilled. This conclusion is also crucial for doing hypothesis testing, because the MLR6 normallyty distribtuion has to be true to do this. This means that we can use a t-statistic as well as an F-statistic for drawing inference.


We can also do this with a histrogram where the results are the same: the residuals seems to be normal distributed, however the histrogram contains some outliers which can infer with the model:
```{r}
hist(residuals)
```

And then manually with a jarque bera test:
```{r}
jarque.bera.test(reg$residuals)
```
$$H0: u \sim N(0,\sigma^2) $$
The p-value is below the 5% significance level meaning that we reject H0, and thereby the residuals are not normally distributed. This contradicts the above conclusion with the graphical control of the model. Here the conclusion was that the residuals where normally distributed.


Next we make a plot to test for MLR5 to test for (homoscedacity).
```{r}
plot(reg, 3)
```
Since the standardized residuals are equally sread around the horizontal line (the predicters), which indicates that the model exhibits homoskedasticity. They are randomly spread meaning that the distance between theese and the horizontal line does not increase with the fitted values.

Lastly we look at if there are relevant outliers in the model which need to be accounted for:

```{r}
plot(reg, 5)
```
It seems that there is no outliers which have a major impact on the results.
This can be concluded using the Cook's distance lines which are not even present in the above, meaning that there are no outliers of impact.

#3. Test for heteroskedasticitet vha. Breusch-Pagan-testet og specialudgaven af White-testet.

```{r}
residuals <- resid(reg)
residuals_squared <- residuals^2
```

```{r}
res_mod <- lm(residuals_squared ~ educ + log(salbegin) + male + minority)
summary(res_mod)
```

```{r}
multiple_r_squared <- 0.02923
n <- nrow(data1)
lm_test <- multiple_r_squared*n
lm_test
```

```{r}
p_value_lmtest <- 1-pchisq(lm_test,4)
p_value_lmtest
```

```{r}
library(lmtest)
bptest(reg)
```
- We reject H0, so there is Heteroskedasticity
- We cannot use the graphical model control

We now use the White-Test: 

```{r}
y_hat <- reg$fitted.values
y_hatsquared <- reg$fitted.values^2
white_test <- lm(residuals_squared ~ y_hat + y_hatsquared)
summary(white_test)
```
- We reject H0 again!

#4. Beregn robuste standardfejl for modellen og sammenlign med resultaterne i spørgsmål 1.

- Vi bruger R til at udregne dem, men vi skal også have formlen med, så dden kan forklares: 

```{r}
reg_robust <- coeftest(reg, vcov = vcovHC(reg, type = "HC0"))
screenreg(list(OLS =reg, OLS_robust_se =reg_robust), digits =4)
```
- Here we can compare our standard errors with our robust standard errors
- Not so big a differene
- The results does not change
- Heterosekdasticity is not very strong in this example


#5. Test hypotesen H0 : β2 = 1 mod alternativet H1 : β2 ̸= 1.
```{r}
t_stat<-(0.82180-1)/0.0374
t_stat
c<-qt(0.05, n-1)
c
p_value<- pt(t_stat, n-1)
p_value
```
- We reject the Null Hypothesis
- Beta_2 is statistically different from 1
- We reject at a 5 pct. significance level 

#6. Test hypotesen H0 : β3 = β4 = 0.

```{r}
reg_restricted <- lm(log(salary)~educ+log(salbegin))
r2_un <- summary(reg)$r.squared
r2_r <- summary(reg_restricted)$r.squared
df <- nrow(data1)-4-1
```

```{r}
#Here we are obtaining our F-statistic
f_stat <- ((r2_un - r2_r)/2)/((1 - r2_un)/df)
f_critical <- qf(0.95, 2, df)
p_value_f <- 1-pf(f_stat, 2, df)
```
- We reject our Null hypothesis 
- Beta_3 and beta_4 are statistically different from 0
- We cxan also test it with a function in R:

```{r}
library(foreign)
library(car)
myh0 <- c("male=0", "minority=0")
linearHypothesis(reg, myh0)
```

#7. Estimer modellen vha. FGLS og kommenter på resultaterne

```{r}
logu_2 <- log(resid(reg)^2) #Obtain residuals from the original regression - We square them - And we log them
```

```{r}
varreg <- lm(logu_2 ~ educ + log(salbegin) + male + minority) #We run a regression on the log_squared_residuals
```

```{r}
w <- exp(fitted(varreg)) #Obtaining the weights
```

```{r}
fgls <- lm(log(salary)~educ+log(salbegin)+male+minority, weight=1/w)
screenreg(list(OLS = reg, FGLS = fgls), digits = 4)
```
- No differencs between the estimates, standard errors or significance of the estimates 

#8. Har FGLS estimationen taget højde for al heteroskedasticiteten?

```{r}
fgls_robust <- coeftest(fgls, vcov = vcovHC(fgls, type = "HC0"))
screenreg(list(OLS = reg, FGLS = fgls, FGLS_robust = fgls_robust), digits = 4)
```
- Here we have obtained robust standard errors for the FGLS 