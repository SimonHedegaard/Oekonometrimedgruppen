---
title: "Exam 1 - Viktor"
author: "Viktor Damm"
date: "2024-05-02"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(sandwich)
library(foreign)
library(texreg)
```

```{r}
data1 <- read.csv("~/Desktop/Oecon/4. semester/Økonometri/Econ Exam/data1.csv")
salary<-data1$salary
educ<-data1$educ
salbegin<-data1$salbegin
male<-data1$male
minority<-data1$minority
```

log(salary) = β0 + β1educ + β2log(salbegin) + β3male + β4minority + u

#1. Estimer modellen vha. OLS. Kommenter på outputtet og fortolk resultaterne.

First of all, we are going the estimate the model using a simple OLS regression in R. This is done with the following code:
```{r}
reg=lm(log(salary)~educ+log(salbegin)+male+minority)
summary(reg)
```
First, we are going to comment of the estimates: 

We can see that if education increases by one unit, then salary increases by 2,3 pct, holding every other parameter fixed. Furthermore, we can see that the estimate is significant down to a 0.1 pct. significance level. Next, we can see that a 1 pct. increase in beginning salaly leads to a 0,82 pct. increase in salary. This estimate is significant down to a 0.1 pct. significance level, as well. We have also included two dummy variables in our regression, male and minority. The estimate on male is 0.04, meaning that a male gets 4% higher salary than females (which is the reference group) holding everything else fixed. The estimate on minority is -0.04, which means that a minority gets 4% lower salary than non-minorities (which is the reference group) holding everything else fixed. Both of these estiamtes are significant down to a 5 pct. significance level. 

We can observe that all variables have the expected sign on the estimates. Further, the R-squared and adjusted R-squared are both around 80 pct, meaning that the model fits 80% of the data. The R-square indicates how well the model fits the data, meaning that it is a meausre of how good the model is. The adjusted R-sqaure also takes the number of independent variables into account. The F-statistic in the model is 481,3 with a p-value of 2.2e-16, meaning that the varibles are jointly significant. 

#Opgave 2. - Udfør grafisk modelkontrol

To answer the quastion we make 4 different graphic model control, because we find theese the most interesting for doing graphical control of the model:
We make graphical control of the model to conclude wether the Gauss Markov assumptions are fullfiled.

Firstly we plot the residuals agains the fitted values, to look at if it seems that there is a non-linear relationsship in the model:

```{r}
plot(reg, 1)
```
Since the dots are equally spread around a horizontal line, which indicates that we dont have any non-linear relationship, which we should account. 
This is a sign of a well specified model, which means that there is no variables which should be included in quadratic form and thereby in a non-linear form.


Next we look at the distribution of the error term. We do this by plotting the standard residuals against the theoretical quantiles, and also in a histrogram of residuals. Lastly we test it manually using the jarque bera test:

```{r}
plot(reg, 2)
```
It seems that the residuals fit nicely around the dottet line, which means that they are normally distributed. This indicates that MLR 6 is fullfilled. This conclusion is also crucial for doing hypothesis testing, because the MLR6 normallyty distribtuion has to be true to do this. This means that we can use a t-statistic as well as an F-statistic for drawing inference.


We can also do this with a histrogram where the results are the same: the residuals seems to be normal distributed, however the histrogram contains some outliers which can infer with the model:
```{r}
hist(residuals)
```

And then manually with a jarque bera test:
```{r}
jarque.bera.test(reg$residuals)
```
$$H0: u \sim N(0,\sigma^2) $$
The p-value is below the 5% significance level meaning that we reject H0, and thereby the residuals are not normally distributed. This contradicts the above conclusion with the graphical control of the model. Here the conclusion was that the residuals where normally distributed.


Next we make a plot to test for MLR5 to test for (homoscedacity).
```{r}
plot(reg, 3)
```
Since the standardized residuals are equally sread around the horizontal line (the predicters), which indicates that the model exhibits homoskedasticity. They are randomly spread meaning that the distance between theese and the horizontal line does not increase with the fitted values.

Lastly we look at if there are relevant outliers in the model which need to be accounted for:

```{r}
plot(reg, 5)
```
It seems that there is no outliers which have a major impact on the results.
This can be concluded using the Cook's distance lines which are not even present in the above, meaning that there are no outliers of impact.

#3. Test for heteroskedasticitet vha. Breusch-Pagan-testet og specialudgaven af White-testet.

If the model exhibits heteroskedasticity, then the variance of the error term is not constant for all independent variables. As mentioned above, this would have some implications for our inference tests. To test for heteroskedasticity we peform the Breusch-Pagan-test and White test manually. The formula that we are going to use is the following: 

$$LM = R^2_{\hat{u}[u]} . n$$

The hypotheses we test for is: 

$$H_0: Var(u|x_1 ,x_2,...,x_k) = \sigma^2$$ 
$$H_1: Var(u|x_1 ,x_2,...,x_k) \neq \sigma^2$$

First of all, we are going to obtain the residuals from our original regression model i question 1 (reg), and then we will square them. The squared residuals works as a proxy for the variance of the error term. 
```{r}
residuals <- resid(reg)
residuals_squared <- residuals^2
```

Then we regress the squared residuals on all the independent variables from the original model, which is: Education, log of salary begin, male and minority. 
```{r}
res_mod <- lm(residuals_squared ~ educ + log(salbegin) + male + minority)
summary(res_mod)
```

Then we obtain the R-squared from the model above (res_model) and define the number of observations (n). Then we can simply use the formula given in the beginning to calculate our lm-test. 
```{r}
multiple_r_squared <- 0.02923
n <- nrow(data1)
lm_test <- multiple_r_squared*n
lm_test
```

Our LM-test i 13,9. To see whether we reject our H0 or not, we calculate the p-value for a chi-square distribution with a lm-test of 13,9. This is done in the code below.
```{r}
p_value_lmtest <- 1-pchisq(lm_test,4)
p_value_lmtest
```

Our p-value is 0.007, which is below a significance level of 5 pct., which means that we clearly rejects the H0, meaning that the model exhibits heteroskedasticity. This contradicts the above conclusion in question 2 with the graphical control of the model. Here the conclusion was that the model exhibited homoskedasticity. To test whether our results are correct, we can perform the Breusch-Pagan test in R automatically. This is done below, and the results correspond to manually performed results. 
```{r}
library(lmtest)
bptest(reg)
```

Lastly, we use the White-Test to test for heteroskedasticity as well. The White-test tests whether a relationship between the independent variables has a influence on the variance of the error term. For this we will use the simple white test, which is tested by the regression model of the following form: 

$$\hat{u}^2=\delta_0+\delta_1\hat{y}+\delta_2\hat{y}^2+error$$
Then we will use the same formula and hypotheses as we did with the Breasch-Pagan test.

$$LM = R^2_{\hat{u}[u]} . n$$

The hypotheses we test for is: 

$$H_0: Var(u|x_1 ,x_2,...,x_k) = \sigma^2$$

$$H_1: Var(u|x_1 ,x_2,...,x_k) \neq \sigma^2$$

First of all, we are going to obtain the fitted values from the original regression model (reg). Then we square the fitted valus. 
```{r}
y_hat <- reg$fitted.values
y_hatsquared <- reg$fitted.values^2
```

Then we are going to do the regression based on the equation above. We then obtain the R-squared from the summary of the model to calculate the LM-test. 
```{r}
white_test <- lm(residuals_squared ~ y_hat + y_hatsquared)
summary(white_test)
multiple_r_squared_white <- 0.02923
```

We calculate the LM-test with the following code:
```{r}
lm_test_white <- multiple_r_squared_white*n
lm_test_white
```

Our LM-test is 13,9, and then we calculate the p-value to determine whether we reject the H0 or not. 

```{r}
p_value_lmtest_white <- 1-pchisq(lm_test_white,4)
p_value_lmtest_white
```

Our p-value is, as in the Breusch-Pagan test, 0.007, which is below a significance level of 5 pct., which means that we clearly rejects the H0, meaning that the model exhibits heteroskedasticity. The results are the same for the Breusch-Pagan test and the simple White test. 

#4. Beregn robuste standardfejl for modellen og sammenlign med resultaterne i spørgsmål 1.

- Vi bruger R til at udregne dem, men vi skal også have formlen med, så dden kan forklares: 

```{r}
reg_robust <- coeftest(reg, vcov = vcovHC(reg, type = "HC0"))
screenreg(list(OLS =reg, OLS_robust_se =reg_robust), digits =4)
```
- Here we can compare our standard errors with our robust standard errors
- Not so big a differene
- The results does not change
- Heterosekdasticity is not very strong in this example


#5. Test hypotesen H0 : β2 = 1 mod alternativet H1 : β2 ̸= 1.
```{r}
t_stat<-(0.82180-1)/0.0374
t_stat
c<-qt(0.05, n-1)
c
p_value<- pt(t_stat, n-1)
p_value
```
- We reject the Null Hypothesis
- Beta_2 is statistically different from 1
- We reject at a 5 pct. significance level 

#6. Test hypotesen H0 : β3 = β4 = 0.

```{r}
reg_restricted <- lm(log(salary)~educ+log(salbegin))
r2_un <- summary(reg)$r.squared
r2_r <- summary(reg_restricted)$r.squared
df <- nrow(data1)-4-1
```

```{r}
#Here we are obtaining our F-statistic
f_stat <- ((r2_un - r2_r)/2)/((1 - r2_un)/df)
f_critical <- qf(0.95, 2, df)
p_value_f <- 1-pf(f_stat, 2, df)
```
- We reject our Null hypothesis 
- Beta_3 and beta_4 are statistically different from 0
- We cxan also test it with a function in R:

```{r}
library(foreign)
library(car)
myh0 <- c("male=0", "minority=0")
linearHypothesis(reg, myh0)
```

#7. Estimer modellen vha. FGLS og kommenter på resultaterne

```{r}
logu_2 <- log(resid(reg)^2) #Obtain residuals from the original regression - We square them - And we log them
```

```{r}
varreg <- lm(logu_2 ~ educ + log(salbegin) + male + minority) #We run a regression on the log_squared_residuals
```

```{r}
w <- exp(fitted(varreg)) #Obtaining the weights
```

```{r}
fgls <- lm(log(salary)~educ+log(salbegin)+male+minority, weight=1/w)
screenreg(list(OLS = reg, FGLS = fgls), digits = 4)
```
- No differencs between the estimates, standard errors or significance of the estimates 

#8. Har FGLS estimationen taget højde for al heteroskedasticiteten?

```{r}
fgls_robust <- coeftest(fgls, vcov = vcovHC(fgls, type = "HC0"))
screenreg(list(OLS = reg, FGLS = fgls, FGLS_robust = fgls_robust), digits = 4)
```
- Here we have obtained robust standard errors for the FGLS 