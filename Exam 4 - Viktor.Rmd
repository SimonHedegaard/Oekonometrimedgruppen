---
title: "Exam 4 - Viktor"
author: "Viktor Damm"
date: "2024-05-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list=ls())
```

```{r}
library(readr)
library(AER)
library(foreign)
library(car)
library(sandwich)
library(texreg)
data4 <- read_csv("data4.csv")
participation <- data4$participation
income <- data4$income
age <- data4$age
agesq <- data4$agesq
educ <- data4$educ
youngkids <- data4$youngkids
oldkids <- data4$oldkids
foreign <- data4$foreign
n <- nrow(data4)
```

#1.Opstil en lineær regressionsmodel for participation, hvor du bruger de beskrevne forklarende variable.

$$participation = \beta_0 + \beta_1*income + \beta_2*age + \beta_3*agesq + \beta_4*educ + \beta_5*youngkids + \beta_6*oldkids + \beta_7*foreign$$
##(a) Estimer modellen vha. OLS og kommenter på resultaterne.

First, we are going to estimate the model, using a OLS regression. This is done with the following code: 
```{r}
ols <- lm(participation ~ income + age + agesq + educ + youngkids + oldkids + foreign)
summary(ols)
```

##(b) Test om den partielle effekt af uddannelse er forskellig fra nul.

To test whether the partial effect of education is different from zero, we will test the following hypotheses with the following formula: 

$$H0:β_4 = 0$$
$$H1:β_4 \neq0$$
 
$$
t = \frac{\hat{\beta}_j - \beta_j}{se(\hat{\beta}_j)}
$$
This is done in the following code, where we also have calculated the critical value and the p-value: 
```{r}
df <- summary(ols)$df[2]
t_stat<-0.0067725/0.0059615
t_stat
c<-qt(1-0.05, n-1)
c
p_value <- 2*pt(-abs(t_stat), df)
p_value
```

We can see that out t-statistic is below our critical value, meaning that we fail to reject our H0 at a 5% significance level. This can also be seen at our p-value, which is 0,26, meaning what we wouldn't even reject our H0 at a 25% significance level. This means that education is not statistically different from 0. 

##(c) Test om den partielle effekt af alder er forskellig fra nul.

For this, we assume that we have to test the jointly significance of both age and age_squared. 

We are testing the following hypotheses: 

$$H0:\beta_2 = \beta_3 = 0$$
$$H1:\beta_2 = \beta_3 \neq0$$

To to test the joint hypothesis, we have to use a F-test. The first formula below is the generel formula har obtaning the F-statistic. The second one is a simplified version, which is the one that we will use. 

$$
F = \frac{\left(SSR_r - SSR_{ur}\right)/q}{SSR_{ur}/\left(n - k - 1\right)}
$$
$$F=\frac{(r^2_{ur}-r^2_{r})/q}{(1-r^2_{ur})/(n-k-1)}$$

First we will obtain the restricted model. If we were to reject our H0, then the variables would have no impact on our dependent variable. Therefore, our restricted model has excluded the variables that we are testing for. Our unrestricted is the original model from question a. We can furthermore observe that we have to obtain our r_squared from the restricted model as well as the unrestricted model. The q is the number of variables excluded from the unrestricted model. All have been calculated in the following code.

```{r}
ols_restricted <- lm(participation ~ income + educ + youngkids + oldkids + foreign)
r2_un <- summary(ols)$r.squared
r2_r <- summary(ols_restricted)$r.squared
df <- nrow(data4)-7-1
```

In the following code we are plotting the results into our F-statistic formula and calculating the cirtical value as well as the p-value.

```{r}
f_stat <- ((r2_un - r2_r)/2)/((1 - r2_un)/df)
f_critical <- qf(0.95, 2, df)
p_value_f <- 1-pf(f_stat, 2, df)
f_stat
f_critical
p_value_f
```

Our critical value is 3, and our t-statistic of 31,87 is above this value. This means that we reject our H0 at a 5% significance level. This can also be seen looking at our p-value, which is a very small number. This is the probability getting a F-statistic that is more extreme, and therefore the lowest significance level at which we will reject out H0. This means that beta_2 and beta_3 are statistically different from 0. 

#2. Opstil både en logit- og en probit-model for participation hvor du bruger de beskrevne forklarende variable.

##(a) Estimer modellerne.

The models are being estimated, using the following code:
```{r}
logit <- glm(participation ~ income + age + agesq + educ + youngkids + oldkids + foreign,
family = binomial(link = "logit"))
probit <- glm(participation ~ income + age + agesq + educ + youngkids + oldkids + foreign,
family = binomial(link = "probit"))
screenreg(list( Logit = logit, Probit = probit), digits = 4)
```

##(b) Test om den partielle effekt af uddannelse er forskellig fra nul.

For the logic model:
```{r}
t_stat_logit <- 0.0386/0.0302
t_stat_logit
c<-qt(1-0.05, n-1)
c
p_value_logit <- 2*pt(-abs(t_stat_logit), df)
p_value_logit
```

For the probit model: 
```{r}
t_stat_probit <- 0.0231/0.0181
t_stat_probit
c<-qt(1-0.05, n-1)
c
p_value_probit <- 2*pt(-abs(t_stat_probit), df)
p_value_probit
```

##(c) Test om den partielle effekt af alder er forskellig fra nul vha. et likelihoodratio-test.

The logit model:
```{r}
## Pseudo R^2
logitR <- glm(participation ~ income + educ + youngkids + oldkids + foreign,
family = binomial(link = "logit"))
lr_logit <- 2 * (logLik(logit) - logLik(logitR))
lr_logit # The df are not true - these are really 2
pval_logit <- pchisq(lr_logit, df = 2, lower.tail = F)
pval_logit # Suggests surely that we must reject H_0 (as expected)
```

The probit model:
```{r}
## Pseudo R^2
probitR <- glm(participation ~ income + educ + youngkids + oldkids + foreign,
family = binomial(link = "probit"))
lr_probit <- 2 * (logLik(probit) - logLik(probitR))
lr_probit # The df are not true - these are really 2
pval_probit <- pchisq(lr_probit, df = 2, lower.tail = F)
pval_probit # Suggests surely that we must reject H_0 (as expected)
```

#3. Vi vil gerne sammenligne den partielle effekt af income på tværs af modellerne. Beregn average partial effect (APE) og kommenter på resultaterne.

In question 2 we concluded that we couldn't interpret the results that the logit and probit model provided. The problem with the interpretation of the logit and probit models is that the partial effect of the different variables depend not only on the betas relating to a particular variable. It also depends on whether the variable is continuous or discrete and on all the other independent variables. You are not only looking at one effect in isolation, but an overall effect. LPM only looks at the isolated effect of the independent variable, whereas this also takes into account the remaining equation. 

Since the interpretation depends on the entire equation, then we need to make an assumption on, which values the other parameters should take. In this case, we use the Average Partial Effect, meaning that it takes the partial effect for all observations and then takes the average of those effects. Furthermore, the variable income is continuous in this case. To get the partial affects using APE, we use the following codes in R. We are doing it for the probit and logit model. 

```{r}
library(mfx)
## Average Partial Effect - logit
ape_logit <- logitmfx(logit, data = data4, atmean = F)
## Average Partial Effect - probit
ape_probit <- probitmfx(probit, data = data4, atmean = F)
```

Now we are able to interpret the results from the logit and probit model. Next, we will compare the results from the logit and probit model with the original OLS regression. 
```{r}
screenreg(list(OLS = ols, Logit_APE = ape_logit, Probit_APE = ape_probit), digits = 4)
```

From the comparision we can see that the estimate on income does not differ much across the models. The estiamte from the OLS model was -0.0035 and the estimate from the Logit and Probit model was -0.0046. The estimates are significant in all models. It seems that income has a higher affect on the participation rate when using the probit and logit models. 

#4. Vi vil gerne sammenligne den partielle effekt af foreign på tværs af modellerne. Beregn APE og kommenter på resultaterne.

The procedure in this question is similar to the above. We need to convert the variable foreign, so we can interpret the results. In generel, we could use the results from the above model, where we used APE. However, when R calculates the partial effets using APE, it assumes that we are dealing with continous varialbes only. Foreign are, however, a binary variable, and there it is a discrete varible. Therefore, we can not directly use the ressults from above, but we have to use the following formula: 

$$$$

This says that, we first treat the variable as 1 and then as 0. Then, we are going to subtract the two, and we can wee the effect, when the binary variable changes from 0 til 1. 

```{r}
cdata <- cbind(1, as.matrix(data4[, c("income", "age", "agesq", "educ",
"youngkids", "oldkids", "foreign")]))
cdata1 <- cdata
cdata1[, 8] <- 1
cdata2 <- cdata
cdata2[, 8] <- 0
pcoef <- probit$coefficients
mean(pnorm(cdata1 %*% pcoef) - pnorm(cdata2 %*% pcoef))
```

This means that being a foreign increases the probability of being in the labor force with 0,249. This result is rather similar to the results from the original OLS regression, where the esimtate where  0.2572. Furthermore, the results is similar to the ones we got with the probit and logit models in the above question using APE. 

#5. Hvorfor er APE at foretrække frem for partial effect at the average (PEA)?

#6. Sammenlign modellernes evne til at prædiktere ved at beregne percent correctly predicted for hver model.

```{r}
## Percently Correctly Predicted
y <- data4["participation"]
olspred <- 100 * mean((ols$fitted > 0.5) == y)
logitpred <- 100 * mean((logit$fitted > 0.5) == y)
probitpred <- 100 * mean((probit$fitted > 0.5) == y)
print(c(olspred, logitpred, probitpred))
```




