---
title: "Viktor"
author: "Viktor Damm"
date: "2024-04-11"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Exam 2: 

```{r}
library(readr)
data2 <- read_csv("data2.csv")
salary <- data2$salary
logsalary <- data2$lsalary
educ <- data2$educ
salbegin <- data2$salbegin
logsalbegin <- data2$lsalbegin
male <- data2$male
minority <- data2$minority
```

#Opgave 1: 

```{r}
model1 <- lm(salary ~ educ + salbegin + male + minority)
model2 <- lm(logsalary ~ educ + logsalbegin + male + minority)
screenreg(list(Model1 = model1, Model2 = model2), digits = 4)
```

Notes: 
Model 1: 
- One more year of education increases the salary by approximately 1000$, holding the other variables fixed. 
- A starting salary increase by 1000$ would increase salary by 1600 dollar yearly, holding other varibles fixed.
- If you are a male, then your salary would be 1800 dollar higher yearly than the reference group female, holding other variables fixed.
- If you are a minotiry, then your salary would be 1700 dollar higher yearly than the reference group non minority, holding other variables fixed.
- The coeffecients of education, salarybegin and male are statistically significant up to the 5% significance level. Minority has become statistically significant, when having a significance level of 5%. 
- The adjusted R^2 means, that 80% of the model is explained by the provided independent variables. 

Model2:
- One more year of education increases the salary by approximately 2,3%, holding the other variables fixed. 
- A starting salary increase by 1% would increase salary by 0,82% yearly, holding other varibles fixed.
- If you are a male, then your salary would be 4,5% higher yearly than the reference group female, holding other variables fixed.
- If you are a minotiry, then your salary would be 4,1% lower yearly than the reference group non minority, holding other variables fixed.
- The coeffecients of education, salarybegin and male are statistically significant up to the 5% significance level. Minority is statisticallyt insignificant, when having a significance level og 5%.  
- The adjusted R^2 means, that 79% of the model is explained by the provided independent variables. 

#Opgave 2

```{r}
plot(model1, which = 1, main = "Salary Model")
plot(model2, which = 1, main = "Log Salary Model")
```
Model1:
-The dots are not equally sread around a horizontal line, indicating, that we perhabs have any non-linear relationship 
- This is a sign of a bad specified model
Model 2:
-The dots are equally sread around a horizontal line, indicating, that we dont have any non-linear relationship 
- This is a sign of well specified model

```{r}
plot(model1, which = 2, main = "Salary Model")
plot(model2, which = 2, main = "Log Salary Model")
```
Model 1: 
- This plot shows, if the residuals are normally distributed
- If they fit nicely around the dottet line, then they are normally distributed 
- Verifies MLR 6
- You could also plot a histogram of the residuals!
Model 2: 
-This one is better!

```{r}
plot(model1, which = 3, main = "Salary Model")
plot(model2, which = 3, main = "Log Salary Model")
```
Model 1: 
- This one checks for homoskedasticity
- -The dots are not equally sread around a horizontal line, indicating, that we have heterosekdasticity
- They arenot randomly spread and doesincrease with the fitted values 
Model 2:
- This one checks for homoskedasticity
- -The dots are equally sread around a horizontal line, indicating, that we have homoskedasticity
- They are randomly spread and does not increase with the fitted values 

```{r}
plot(model1, which = 5, main = "Salary Model")
plot(model2, which = 5, main = "Log Salary Model")
```
Model 1:
- This one check, if we have any outliers of major impact
_ Cook's distance lines are present, meaning that there could be outliers of impact, but there are no outside the lines, so it shouldn't be a problem
Model 2:
- THis one check, if we have any outliers of major impact
_ Cook's distance lines are not even present, meaning that there are no outliers of impact.

- All in all, we prefer the model with salary and salary begin 

#Opgave 3: 

$$y=\beta_0 + \beta_1x_1+\beta_2x_2+...\beta_kx_k+\delta_1\hat{y}^2+\delta_2\hat{y}^3+v$$

$$H_0=\delta_1=\delta_2=0$$
$$H_0=\delta_1=\delta_2\neq0$$

```{r}
y_hat1 <- -6.9323 + 0.9933*educ + 1.6082*salbegin + 1.8309*male - 1.7254*minority
y_hat2 <- 0.84913 + 0.02358*educ + 0.82073*logsalbegin + 0.04547*male - 0.04186*minority
```

```{r}
y_hat1_squared <- y_hat1^2
y_hat1_cubed <- y_hat1^3
y_hat2_squared <- y_hat2^2
y_hat2_cubed <- y_hat2^3
```

```{r}
regreset1 <- lm(salary ~ educ + salbegin + male + minority + y_hat1_squared + y_hat1_cubed)
```

```{r}
regreset2 <- lm(logsalary ~ educ + logsalbegin + male + minority + y_hat2_squared + y_hat2_cubed)
```

```{r}
waldtest(regreset1, vcov = vcov(regreset1), terms = c("y_hat1_squared", "y_hat1_cubed")) #tester manuelt om de hver især er = 0
waldtest(regreset2, vcov = vcov(regreset2), terms = c("y_hat2_squared", "y_hat2_cubed"))   #tester manuelt om de hver især er = 0
```

```{r}
library(lmtest)
resettest(model1)
resettest(model2)
```

#4. Forklar hvorfor det kunne være relevant at medtage educ2 som forklarende variabel i de to modeller. Estimer de to modeller igen hvor educ2 inkluderes (med tilhørende koefficient β5), kommenter kort på outputtet og udfør RESET-testet igen.

```{r}
model1_ny <- lm(salary ~ educ + salbegin + male + minority + I(educ^2))
model2_ny <- lm(logsalary ~ educ + I(educ^2) + logsalbegin + male + minority + I(educ^2))
screenreg(list(Model1_ny = model1_ny, Model2_ny = model2_ny), digits = 4)
```

```{r}
resettest(model1_ny)
resettest(model2_ny)
```

#5. Test hypotesen H0 : β1 = β5 = 0 i begge modeller (fra spørgsmål 4).

```{r}
library(foreign)
library(car)
myh0 <- c("educ=0", "I(educ^2)=0")
linearHypothesis(model1_ny, myh0)
linearHypothesis(model2_ny, myh0)
```

- Educ and Educ^2 are jointly significant in both models!

#6. Kunne der være problemer med målefejl i de to modeller? I hvilke tilfælde vil det udgøre et problem?

We think that 2 variables may have measurement error. 

Education
- Some people may include on the job training, special training, internship and generel training.

Salary/Beginsalary
- Some people may include pensions and employment benefits such as payed lunch, car provided by their company, employment stocks, etc.

But it could also be male and minority

Male
- If one is in doubt.

Minority
- If one believe that they are not a minority.


Problem for the dependent variable:
- Would be a problem if you have measurement error in the dependent variable, then a bias depends whether the cov(u+e,x) is equal or not equal to 0. If it not equal to 0 there may be a measurement error.

Problem for the independent variable:
- Depending on whether we assume that one of the independent variables is a proxy which is correlated with the error term 2 thing can happen:
- If the proxy is not correlated with the measurement error then the estimators will not be biased but the varians of the residuals will be larger.
- If the proxy is correlated with the the measurement error then the estimators will be biased and we would have classical errors in variables (CEV).

#7. Beregn den prædikterede løn,ásalar y, for hver af de to modeller(fra spørgsmål 4) for de 474 observationer. På baggrund af disse, hvilken model vil du så foretrække?

```{r}
library(foreign)
library(ggplot2) # this is a very useful package for data visualisation in R.
X <- data.frame(educ=seq(1,25), salbegin=17, minority=1, male=1)
```

#model 1:
```{r}
c_interval <- predict(model1_ny, X, interval = "confidence") # this computes the CI for the model 
p_interval <- predict(model1_ny, X, interval = "prediction") # this computes the PI for the model
```

```{r}
salary_pred= c_interval[,1] # this extracts the predicted values 
lower_ci = c_interval[,2] #this extracts the lower ci from c_interval 
upper_ci=c_interval[,3] #this extracts the upper ci from c_interval 
lower_pi = p_interval[,2] #this extracts the lower pi from c_interval 
upper_pi=p_interval[,3] #this extracts the upper ci from c_interval
```

```{r}
myplot <- ggplot(data = X, aes(x = X[,1])) +
  geom_line(aes(y = salary_pred, colour = "Predicted")) +
  geom_line(aes(y = lower_ci, colour = "Confidence interval")) +
  geom_line(aes(y = upper_ci, colour = "Confidence interval")) +
  geom_line(aes(y = lower_pi, colour = "Predicted interval")) +
  geom_line(aes(y = upper_pi, colour = "Predicted interval")) +
  scale_color_manual(values = c("Predicted" = "red", "Confidence interval" = "blue", "Predicted interval" = "green"),
                     name = "Lines",  # You can adjust the legend title here
                     labels = c("Predicted", "Confidence interval", "Predicted interval"))  # You can adjust the legend labels here
myplot
```